@article{russakovsky2015imagenet,
  title={{ImageNet} large scale visual recognition challenge},
  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  journal={International journal of computer vision},
  volume={115},
  number={3},
  pages={211--252},
  year={2015},
  publisher={Springer}
}


@article{dijkshoorn2018rijksmuseum,
  title={The Rijksmuseum collection as linked data},
  author={Dijkshoorn, Chris and Jongma, Lizzy and Aroyo, Lora and Van Ossenbruggen, Jacco and Schreiber, Guus and Ter Weele, Wesley and Wielemaker, Jan},
  journal={Semantic Web},
  volume={9},
  number={2},
  pages={221--230},
  year={2018},
  publisher={IOS Press}
}

@inproceedings{concordia2009not,
  title={Not (just) a Repository, nor (just) a Digital Library, nor (just) a Portal: A Portrait of Europeana as an API},
  author={Concordia, Cesare and Gradmann, Stefan and Siebinga, Sjoerd},
  booktitle={World Library and Information Congress: 75th IFLA General Conference and Council},
  year={2009}
}

@article{zimmer2015twitter,
  title={The {T}witter Archive at the Library of Congress: Challenges for information practice and information policy},
  author={Zimmer, Michael},
  journal={First Monday},
  volume={20},
  number={7},
  year={2015}
}

@inproceedings{buolamwini2018gender,
  title={Gender shades: Intersectional accuracy disparities in commercial gender classification},
  author={Buolamwini, Joy and Gebru, Timnit},
  booktitle={Conference on fairness, accountability and transparency},
  pages={77--91},
  year={2018}
}

@article{khaw2019individual,
  title={Individual Differences in the Perception of Probability},
  author={Khaw, Mel Win and Stevens, Luminita and Woodford, Michael},
  journal={Available at SSRN 3446790},
  year={2019}
}

@article{seitsonen2017crowdsourcing,
  title={Crowdsourcing cultural heritage: public participation and conflict legacy in Finland},
  author={Seitsonen, Oula},
  journal={Journal of Community Archaeology \& Heritage},
  volume={4},
  number={2},
  pages={115--130},
  year={2017},
  publisher={Taylor \& Francis}
}

@article{baldwin1968poverty,
  title={Poverty and politics; the rise and decline of the Farm Security Administration},
  author={Baldwin, Sidney},
  year={1968},
  publisher={University of North Carolina},
  url={https://www.uncpress.org/book/9780807896129/}
}

@book{trachtenberg1990reading,
  title={Reading American Photographs: Images as History-Mathew Brady to Walker Evans},
  author={Trachtenberg, Alan},
  year={1990},
  publisher={Macmillan},
  address={London, England},
  url={https://us.macmillan.com/books/9780374522490}
}

@article{vinyals2016show,
  title={Show and tell: Lessons learned from the 2015 {MS COCO} image captioning challenge},
  author={Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={39},
  number={4},
  pages={652--663},
  year={2016},
  publisher={IEEE}
}

@inproceedings{mason-charniak-2012-apples,
    title = "Apples to Oranges: Evaluating Image Annotations from Natural Language Processing Systems",
    author = "Mason, Rebecca  and
      Charniak, Eugene",
    booktitle = "Proceedings of the 2012 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2012",
    address = "Montr{\'e}al, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N12-1018",
    pages = "172--181",
    abstract = "We examine evaluation methods for systemsthat automatically annotate images using co-occurring text.  We compare previous datasetsfor  this  task  using  a  series  of  baseline  mea-sures inspired by those used in information re-trieval,  computer vision,  and extractive sum-marization.   Some of our baselines match orexceed  the  best  published  scores  for  thosedatasets. These results illuminate incorrect as-sumptions  and  improper  practices  regardingpreprocessing, evaluation metrics, and the col-lection  of  gold  image  annotations.   We  con-clude with a list of recommended practices forfuture  research  combining  language  and  vi-sion processing techniques."
}

@InProceedings{BATRA18.725,
  author = {Vishwash Batra and Yulan He and George Vogiatzis},
  title = "{Neural Caption Generation for News Images}",
  booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)},
  year = {2018},
  month = {May 7-12, 2018},
  address = {Miyazaki, Japan},
  editor = {Nicoletta Calzolari (Conference chair) and Khalid Choukri and Christopher Cieri and Thierry Declerck and Sara Goggi and Koiti Hasida and Hitoshi Isahara and Bente Maegaard and Joseph Mariani and Hélène Mazo and Asuncion Moreno and Jan Odijk and Stelios Piperidis and Takenobu Tokunaga},
  publisher = {European Language Resources Association (ELRA)},
  isbn = {979-10-95546-00-9},
  language = {english}
}

@InProceedings{YOKOTA18.480,
  author = {Masashi Yokota and Hideki Nakayama},
  title = "{Augmenting Image Question Answering Dataset by Exploiting Image Captions}",
  booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)},
  year = {2018},
  month = {May 7-12, 2018},
  address = {Miyazaki, Japan},
  editor = {Nicoletta Calzolari (Conference chair) and Khalid Choukri and Christopher Cieri and Thierry Declerck and Sara Goggi and Koiti Hasida and Hitoshi Isahara and Bente Maegaard and Joseph Mariani and Hélène Mazo and Asuncion Moreno and Jan Odijk and Stelios Piperidis and Takenobu Tokunaga},
  publisher = {European Language Resources Association (ELRA)},
  isbn = {979-10-95546-00-9},
  language = {english}
  }

@InProceedings{GATT18.226,
  author = {Albert Gatt and Marc Tanti and Adrian Muscat and Patrizia Paggio and Reuben A Farrugia and Claudia Borg and Kenneth Camilleri and Mike Rosner and Lonneke Van der Plas},
  title = "{Face2Text: Collecting an Annotated Image Description Corpus for the Generation of Rich Face Descriptions}",
  booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)},
  year = {2018},
  month = {May 7-12, 2018},
  address = {Miyazaki, Japan},
  editor = {Nicoletta Calzolari (Conference chair) and Khalid Choukri and Christopher Cieri and Thierry Declerck and Sara Goggi and Koiti Hasida and Hitoshi Isahara and Bente Maegaard and Joseph Mariani and Hélène Mazo and Asuncion Moreno and Jan Odijk and Stelios Piperidis and Takenobu Tokunaga},
  publisher = {European Language Resources Association (ELRA)},
  isbn = {979-10-95546-00-9},
  language = {english}
  }

@InProceedings{HOLLINK16.19,
  author = {Laura Hollink and Adriatik Bedjeti and Martin van Harmelen and Desmond Elliott},
  title = {A Corpus of Images and Text in Online News},
  booktitle = {Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016)},
  year = {2016},
  month = {may},
  date = {23-28},
  location = {Portorož, Slovenia},
  editor = {Nicoletta Calzolari (Conference Chair) and Khalid Choukri and Thierry Declerck and Sara Goggi and Marko Grobelnik and Bente Maegaard and Joseph Mariani and Helene Mazo and Asuncion Moreno and Jan Odijk and Stelios Piperidis},
  publisher = {European Language Resources Association (ELRA)},
  address = {Paris, France},
  isbn = {978-2-9517408-9-1},
  language = {english}
 }

 @inproceedings{platt1999using,
  title={Using analytic QP and sparseness to speed training of support vector machines},
  author={Platt, John C},
  booktitle={Advances in neural information processing systems},
  pages={557--563},
  year={1999}
}

@inproceedings{lin2015bilinear,
  title={Bilinear {CNN} models for fine-grained visual recognition},
  author={Lin, Tsung-Yu and RoyChowdhury, Aruni and Maji, Subhransu},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1449--1457},
  year={2015}
}

@book{tagg2009disciplinary,
  title={The disciplinary frame: Photographic truths and the capture of meaning},
  author={Tagg, John},
  year={2009},
  publisher={U of Minnesota Press}
}


@inproceedings{caesar2018coco,
  title={{COCO}-stuff: Thing and stuff classes in context},
  author={Caesar, Holger and Uijlings, Jasper and Ferrari, Vittorio},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1209--1218},
  year={2018}
}

@inproceedings{mensink2014rijksmuseum,
  title={The {R}ijksmuseum challenge: Museum-centered visual recognition},
  author={Mensink, Thomas and Van Gemert, Jan},
  booktitle={Proceedings of International Conference on Multimedia Retrieval},
  pages={451--454},
  year={2014}
}

@article{weibel1997dublin,
  title={The {D}ublin {C}ore: a simple content description model for electronic resources},
  author={Weibel, Stuart},
  journal={Bulletin of the American Society for Information Science and Technology},
  volume={24},
  number={1},
  pages={9--11},
  year={1997},
  publisher={Wiley Online Library}
}

@article{alexiev2018museum,
  title={Museum Linked Open Data: Ontologies, Datasets, Projects},
  author={Alexiev, Vladimir},
  journal={Digital Presentation and Preservation of Cultural and Scientific Heritage},
  number={VIII},
  pages={19--50},
  year={2018},
  publisher={Институт по математика и информатика-Българска академия на науките}
}



@inproceedings{agt2018semantic,
  title={Semantic Annotation and Automated Extraction of Audio-Visual Staging Patterns in Large-Scale Empirical Film Studies.},
  author={Agt-Rickauer, Henning and Hentschel, Christian and Sack, Harald},
  booktitle={SEMANTICS Posters\&Demos},
  year={2018}
}

@inproceedings{kirillov2019panoptic,
  title={Panoptic segmentation},
  author={Kirillov, Alexander and He, Kaiming and Girshick, Ross and Rother, Carsten and Doll{\'a}r, Piotr},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={9404--9413},
  year={2019}
}

@inproceedings{alikhani-etal-2019-cite,
    title = "{CITE}: A Corpus of Image-Text Discourse Relations",
    author = "Alikhani, Malihe  and
      Nag Chowdhury, Sreyasi  and
      de Melo, Gerard  and
      Stone, Matthew",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1056",
    doi = "10.18653/v1/N19-1056",
    pages = "570--575",
    abstract = "This paper presents a novel crowd-sourced resource for multimodal discourse: our resource characterizes inferences in image-text contexts in the domain of cooking recipes in the form of coherence relations. Like previous corpora annotating discourse structure between text arguments, such as the Penn Discourse Treebank, our new corpus aids in establishing a better understanding of natural communication and common-sense reasoning, while our findings have implications for a wide range of applications, such as understanding and generation of multimodal documents.",
}

@inproceedings{singhal-etal-2019-learning,
    title = "Learning Multilingual Word Embeddings Using Image-Text Data",
    author = "Singhal, Karan  and
      Raman, Karthik  and
      ten Cate, Balder",
    booktitle = "Proceedings of the Second Workshop on Shortcomings in Vision and Language",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-1807",
    doi = "10.18653/v1/W19-1807",
    pages = "68--77",
    abstract = "There has been significant interest recently in learning multilingual word embeddings {--} in which semantically similar words across languages have similar embeddings. State-of-the-art approaches have relied on expensive labeled data, which is unavailable for low-resource languages, or have involved post-hoc unification of monolingual embeddings. In the present paper, we investigate the efficacy of multilingual embeddings learned from weakly-supervised image-text data. In particular, we propose methods for learning multilingual embeddings using image-text data, by enforcing similarity between the representations of the image and that of the text. Our experiments reveal that even without using any expensive labeled data, a bag-of-words-based embedding model trained on image-text data achieves performance comparable to the state-of-the-art on crosslingual semantic similarity tasks.",
}

@misc{pixplot,
  author = {Douglas Duhaime},
  title = {{PixPlot}: Visualize large image collections with {WebGL}},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/YaleDHLab/pix-plot}}
}

@misc{wu2019detectron2,
  author =       {Yuxin Wu and Alexander Kirillov and Francisco Massa and
                  Wan-Yen Lo and Ross Girshick},
  title =        {Detectron2},
  howpublished = {\url{https://github.com/facebookresearch/detectron2}},
  year =         {2019}
}

@inproceedings{wang-etal-2018-object,
    title = "Object Counts! Bringing Explicit Detections Back into Image Captioning",
    author = "Wang, Josiah  and
      Madhyastha, Pranava Swaroop  and
      Specia, Lucia",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-1198",
    doi = "10.18653/v1/N18-1198",
    pages = "2180--2193",
    abstract = "The use of explicit object detectors as an intermediate step to image captioning {--} which used to constitute an essential stage in early work {--} is often bypassed in the currently dominant end-to-end approaches, where the language model is conditioned directly on a mid-level image embedding. We argue that explicit detections provide rich semantic information, and can thus be used as an interpretable representation to better understand why end-to-end image captioning systems work well. We provide an in-depth analysis of end-to-end image captioning by exploring a variety of cues that can be derived from such object detections. Our study reveals that end-to-end image captioning systems rely on matching image representations to generate captions, and that encoding the frequency, size and position of objects are complementary and all play a role in forming a good image representation. It also reveals that different object categories contribute in different ways towards image captioning.",
}

@inproceedings{gella-keller-2018-evaluation,
    title = "An Evaluation of Image-Based Verb Prediction Models against Human Eye-Tracking Data",
    author = "Gella, Spandana  and
      Keller, Frank",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-2119",
    doi = "10.18653/v1/N18-2119",
    pages = "758--763",
    abstract = "Recent research in language and vision has developed models for predicting and disambiguating verbs from images. Here, we ask whether the predictions made by such models correspond to human intuitions about visual verbs. We show that the image regions a verb prediction model identifies as salient for a given verb correlate with the regions fixated by human observers performing a verb classification task.",
}

@inproceedings{van-miltenburg-etal-2018-measuring,
    title = "Measuring the Diversity of Automatic Image Descriptions",
    author = "van Miltenburg, Emiel  and
      Elliott, Desmond  and
      Vossen, Piek",
    booktitle = "Proceedings of the 27th International Conference on Computational Linguistics",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/C18-1147",
    pages = "1730--1741",
    abstract = "Automatic image description systems typically produce generic sentences that only make use of a small subset of the vocabulary available to them. In this paper, we consider the production of generic descriptions as a lack of diversity in the output, which we quantify using established metrics and two new metrics that frame image description as a word recall task. This framing allows us to evaluate system performance on the head of the vocabulary, as well as on the long tail, where system performance degrades. We use these metrics to examine the diversity of the sentences generated by nine state-of-the-art systems on the MS COCO data set. We find that the systems trained with maximum likelihood objectives produce less diverse output than those trained with additional adversarial objectives. However, the adversarially-trained models only produce more types from the head of the vocabulary and not the tail. Besides vocabulary-based methods, we also look at the compositional capacity of the systems, specifically their ability to create compound nouns and prepositional phrases of different lengths. We conclude that there is still much room for improvement, and offer a toolkit to measure progress towards the goal of generating more diverse image descriptions.",
}

@inproceedings{shimizu-etal-2018-visual,
    title = "Visual Question Answering Dataset for Bilingual Image Understanding: A Study of Cross-Lingual Transfer Using Attention Maps",
    author = "Shimizu, Nobuyuki  and
      Rong, Na  and
      Miyazaki, Takashi",
    booktitle = "Proceedings of the 27th International Conference on Computational Linguistics",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/C18-1163",
    pages = "1918--1928",
    abstract = "Visual question answering (VQA) is a challenging task that requires a computer system to understand both a question and an image. While there is much research on VQA in English, there is a lack of datasets for other languages, and English annotation is not directly applicable in those languages. To deal with this, we have created a Japanese VQA dataset by using crowdsourced annotation with images from the Visual Genome dataset. This is the first such dataset in Japanese. As another contribution, we propose a cross-lingual method for making use of English annotation to improve a Japanese VQA system. The proposed method is based on a popular VQA method that uses an attention mechanism. We use attention maps generated from English questions to help improve the Japanese VQA task. The proposed method experimentally performed better than simply using a monolingual corpus, which demonstrates the effectiveness of using attention maps to transfer cross-lingual information.",
}


@inproceedings{jiang-etal-2019-reo,
    title = "{REO}-Relevance, Extraness, Omission: A Fine-grained Evaluation for Image Captioning",
    author = "Jiang, Ming  and
      Hu, Junjie  and
      Huang, Qiuyuan  and
      Zhang, Lei  and
      Diesner, Jana  and
      Gao, Jianfeng",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1156",
    doi = "10.18653/v1/D19-1156",
    pages = "1475--1480",
    abstract = "Popular metrics used for evaluating image captioning systems, such as BLEU and CIDEr, provide a single score to gauge the system{'}s overall effectiveness. This score is often not informative enough to indicate what specific errors are made by a given system. In this study, we present a fine-grained evaluation method REO for automatically measuring the performance of image captioning systems. REO assesses the quality of captions from three perspectives: 1) Relevance to the ground truth, 2) Extraness of the content that is irrelevant to the ground truth, and 3) Omission of the elements in the images and human references. Experiments on three benchmark datasets demonstrate that our method achieves a higher consistency with human judgments and provides more intuitive evaluation results than alternative metrics.",
}

@inproceedings{hessel-etal-2019-unsupervised,
    title = "Unsupervised Discovery of Multimodal Links in Multi-image, Multi-sentence Documents",
    author = "Hessel, Jack  and
      Lee, Lillian  and
      Mimno, David",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1210",
    doi = "10.18653/v1/D19-1210",
    pages = "2034--2045",
    abstract = "Images and text co-occur constantly on the web, but explicit links between images and sentences (or other intra-document textual units) are often not present. We present algorithms that discover image-sentence relationships without relying on explicit multimodal annotation in training. We experiment on seven datasets of varying difficulty, ranging from documents consisting of groups of images captioned post hoc by crowdworkers to naturally-occurring user-generated multimodal documents. We find that a structured training objective based on identifying whether collections of images and sentences co-occur in documents can suffice to predict links between specific sentences and specific images within the same document at test time.",
}

@inproceedings{jiang-etal-2019-reo,
    title = "{REO}-Relevance, Extraness, Omission: A Fine-grained Evaluation for Image Captioning",
    author = "Jiang, Ming  and
      Hu, Junjie  and
      Huang, Qiuyuan  and
      Zhang, Lei  and
      Diesner, Jana  and
      Gao, Jianfeng",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1156",
    doi = "10.18653/v1/D19-1156",
    pages = "1475--1480",
    abstract = "Popular metrics used for evaluating image captioning systems, such as BLEU and CIDEr, provide a single score to gauge the system{'}s overall effectiveness. This score is often not informative enough to indicate what specific errors are made by a given system. In this study, we present a fine-grained evaluation method REO for automatically measuring the performance of image captioning systems. REO assesses the quality of captions from three perspectives: 1) Relevance to the ground truth, 2) Extraness of the content that is irrelevant to the ground truth, and 3) Omission of the elements in the images and human references. Experiments on three benchmark datasets demonstrate that our method achieves a higher consistency with human judgments and provides more intuitive evaluation results than alternative metrics.",
}

@inproceedings{nikolaus-etal-2019-compositional,
    title = "Compositional Generalization in Image Captioning",
    author = "Nikolaus, Mitja  and
      Abdou, Mostafa  and
      Lamm, Matthew  and
      Aralikatte, Rahul  and
      Elliott, Desmond",
    booktitle = "Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/K19-1009",
    doi = "10.18653/v1/K19-1009",
    pages = "87--98",
    abstract = "Image captioning models are usually evaluated on their ability to describe a held-out set of images, not on their ability to generalize to unseen concepts. We study the problem of compositional generalization, which measures how well a model composes unseen combinations of concepts when describing images. State-of-the-art image captioning models show poor generalization performance on this task. We propose a multi-task model to address the poor performance, that combines caption generation and image{--}sentence ranking, and uses a decoding mechanism that re-ranks the captions according their similarity to the image. This model is substantially better at generalizing to unseen combinations of concepts compared to state-of-the-art captioning models.",
}

@inproceedings{kiros-etal-2018-illustrative,
    title = "Illustrative Language Understanding: Large-Scale Visual Grounding with Image Search",
    author = "Kiros, Jamie  and
      Chan, William  and
      Hinton, Geoffrey",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-1085",
    doi = "10.18653/v1/P18-1085",
    pages = "922--933",
    abstract = "We introduce Picturebook, a large-scale lookup operation to ground language via {`}snapshots{'} of our physical world accessed through image search. For each word in a vocabulary, we extract the top-$k$ images from Google image search and feed the images through a convolutional network to extract a word embedding. We introduce a multimodal gating function to fuse our Picturebook embeddings with other word representations. We also introduce Inverse Picturebook, a mechanism to map a Picturebook embedding back into words. We experiment and report results across a wide range of tasks: word similarity, natural language inference, semantic relatedness, sentiment/topic classification, image-sentence ranking and machine translation. We also show that gate activations corresponding to Picturebook embeddings are highly correlated to human judgments of concreteness ratings.",
}

@inproceedings{sharma-etal-2018-conceptual,
    title = "Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning",
    author = "Sharma, Piyush  and
      Ding, Nan  and
      Goodman, Sebastian  and
      Soricut, Radu",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-1238",
    doi = "10.18653/v1/P18-1238",
    pages = "2556--2565",
    abstract = "We present a new dataset of image caption annotations, Conceptual Captions, which contains an order of magnitude more images than the MS-COCO dataset (Lin et al., 2014) and represents a wider variety of both images and image caption styles. We achieve this by extracting and filtering image caption annotations from billions of webpages. We also present quantitative evaluations of a number of image captioning models and show that a model architecture based on Inception-ResNetv2 (Szegedy et al., 2016) for image-feature extraction and Transformer (Vaswani et al., 2017) for sequence modeling achieves the best performance when trained on the Conceptual Captions dataset.",
}

@inproceedings{hartmann-sogaard-2018-limitations,
    title = "Limitations of Cross-Lingual Learning from Image Search",
    author = "Hartmann, Mareike  and
      S{\o}gaard, Anders",
    booktitle = "Proceedings of The Third Workshop on Representation Learning for {NLP}",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-3021",
    doi = "10.18653/v1/W18-3021",
    pages = "159--163",
    abstract = "Cross-lingual representation learning is an important step in making NLP scale to all the world{'}s languages. Previous work on bilingual lexicon induction suggests that it is possible to learn cross-lingual representations of words based on similarities between images associated with these words. However, that work focused (almost exclusively) on the translation of nouns only. Here, we investigate whether the meaning of other parts-of-speech (POS), in particular adjectives and verbs, can be learned in the same way. Our experiments across five language pairs indicate that previous work does not scale to the problem of learning cross-lingual representations beyond simple nouns.",
}

@inproceedings{zhao-etal-2019-informative,
    title = "Informative Image Captioning with External Sources of Information",
    author = "Zhao, Sanqiang  and
      Sharma, Piyush  and
      Levinboim, Tomer  and
      Soricut, Radu",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1650",
    doi = "10.18653/v1/P19-1650",
    pages = "6485--6494",
    abstract = "An image caption should fluently present the essential information in a given image, including informative, fine-grained entity mentions and the manner in which these entities interact. However, current captioning models are usually trained to generate captions that only contain common object names, thus falling short on an important {``}informativeness{''} dimension. We present a mechanism for integrating image information together with fine-grained labels (assumed to be generated by some upstream models) into a caption that describes the image in a fluent and informative manner. We introduce a multimodal, multi-encoder model based on Transformer that ingests both image features and multiple sources of entity labels. We demonstrate that we can learn to control the appearance of these entity labels in the output, resulting in captions that are both fluent and informative.",
}

@inproceedings{fan-etal-2019-bridging,
    title = "Bridging by Word: Image Grounded Vocabulary Construction for Visual Captioning",
    author = "Fan, Zhihao  and
      Wei, Zhongyu  and
      Wang, Siyuan  and
      Huang, Xuanjing",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1652",
    doi = "10.18653/v1/P19-1652",
    pages = "6514--6524",
    abstract = "Image Captioning aims at generating a short description for an image. Existing research usually employs the architecture of CNN-RNN that views the generation as a sequential decision-making process and the entire dataset vocabulary is used as decoding space. They suffer from generating high frequent n-gram with irrelevant words. To tackle this problem, we propose to construct an image-grounded vocabulary, based on which, captions are generated with limitation and guidance. In specific, a novel hierarchical structure is proposed to construct the vocabulary incorporating both visual information and relations among words. For generation, we propose a word-aware RNN cell incorporating vocabulary information into the decoding process directly. Reinforce algorithm is employed to train the generator using constraint vocabulary as action space. Experimental results on MS COCO and Flickr30k show the effectiveness of our framework compared to some state-of-the-art models.",
}

@inproceedings{hessel-etal-2015-image,
    title = "Image Representations and New Domains in Neural Image Captioning",
    author = "Hessel, Jack  and
      Savva, Nicolas  and
      Wilber, Michael",
    booktitle = "Proceedings of the Fourth Workshop on Vision and Language",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W15-2807",
    doi = "10.18653/v1/W15-2807",
    pages = "29--39",
    abstract = "We  examine  the  possibility  that  recentpromising   results   in   automatic   captiongeneration  are  due  primarily  to  languagemodels.  By varying image representationquality produced by a convolutional neu-ral  network,  we  find  that  a  state-of-the-art neural captioning algorithm is able toproduce  quality  captions  even  when  pro-vided  with  surprisingly  poor  image  rep-resentations.    We  replicate  this  result  ina new,  fine-grained,  transfer learned cap-tioning domain, consisting of 66K recipeimage/title  pairs.   We  also  provide  someexperiments regarding the appropriatenessof datasets for automatic captioning,  andfind that having multiple captions per im-age  is  beneficial,  but  not  an  absolute  re-quirement"
}

@inproceedings{sadler-etal-2019-neural,
    title = "Can Neural Image Captioning be Controlled via Forced Attention?",
    author = "Sadler, Philipp  and
      Scheffler, Tatjana  and
      Schlangen, David",
    booktitle = "Proceedings of the 12th International Conference on Natural Language Generation",
    month = oct # "{--}" # nov,
    year = "2019",
    address = "Tokyo, Japan",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-8653",
    doi = "10.18653/v1/W19-8653",
    pages = "427--431",
    abstract = "Learned dynamic weighting of the conditioning signal (attention) has been shown to improve neural language generation in a variety of settings. The weights applied when generating a particular output sequence have also been viewed as providing a potentially explanatory insight in the internal workings of the generator. In this paper, we reverse the direction of this connection and ask whether through the control of the attention of the model we can control its output. Specifically, we take a standard neural image captioning model that uses attention, and fix the attention to predetermined areas in the image. We evaluate whether the resulting output is more likely to mention the class of the object in that area than the normally generated caption. We introduce three effective methods to control the attention and find that these are producing expected results in up to 27.43{\%} of the cases.",
}

@inproceedings{chen-etal-2018-attacking,
    title = "Attacking Visual Language Grounding with Adversarial Examples: A Case Study on Neural Image Captioning",
    author = "Chen, Hongge  and
      Zhang, Huan  and
      Chen, Pin-Yu  and
      Yi, Jinfeng  and
      Hsieh, Cho-Jui",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-1241",
    doi = "10.18653/v1/P18-1241",
    pages = "2587--2597",
    abstract = "Visual language grounding is widely studied in modern neural image captioning systems, which typically adopts an encoder-decoder framework consisting of two principal components: a convolutional neural network (CNN) for image feature extraction and a recurrent neural network (RNN) for language caption generation. To study the robustness of language grounding to adversarial perturbations in machine vision and perception, we propose Show-and-Fool, a novel algorithm for crafting adversarial examples in neural image captioning. The proposed algorithm provides two evaluation approaches, which check if we can mislead neural image captioning systems to output some randomly chosen captions or keywords. Our extensive experiments show that our algorithm can successfully craft visually-similar adversarial examples with randomly targeted captions or keywords, and the adversarial examples can be made highly transferable to other image captioning systems. Consequently, our approach leads to new robustness implications of neural image captioning and novel insights in visual language grounding.",
}

@inproceedings{xu2015show,
  title={Show, attend and tell: Neural image caption generation with visual attention},
  author={Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhudinov, Ruslan and Zemel, Rich and Bengio, Yoshua},
  booktitle={International conference on machine learning},
  pages={2048--2057},
  year={2015}
}

@article{guha2016schema,
  title={Schema. org: evolution of structured data on the web},
  author={Guha, Ramanathan V and Brickley, Dan and Macbeth, Steve},
  journal={Communications of the ACM},
  volume={59},
  number={2},
  pages={44--51},
  year={2016},
  publisher={ACM New York, NY, USA}
}

@inproceedings{mcauley2015image,
  title={Image-based recommendations on styles and substitutes},
  author={McAuley, Julian and Targett, Christopher and Shi, Qinfeng and Van Den Hengel, Anton},
  booktitle={Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={43--52},
  year={2015}
}

@article{thompson2017computational,
  title={Computational Cut-Ups: The Influence of Dada},
  author={Thompson, Laure and Mimno, David},
  journal={The Journal of Modern Periodical Studies},
  volume={8},
  number={2},
  pages={179--195},
  year={2017},
  publisher={Penn State University Press}
}

@article{wevers2019visual,
  title={The visual digital turn: Using neural networks to study historical images},
  author={Wevers, Melvin and Smits, Thomas},
  journal={Digital Scholarship in the Humanities},
  year={2019}
}

@inproceedings{aubert2005advene,
  title={Advene: active reading through hypervideo},
  author={Aubert, Olivier and Pri{\'e}, Yannick},
  booktitle={Proceedings of the sixteenth ACM conference on Hypertext and hypermedia},
  pages={235--244},
  year={2005}
}

@misc{w3c,
  title = {Web Annotation Data Model},
  author = {Robert Sanderson and Paolo Ciccarese and Benjamin Young},
  howpublished = {\url{https://www.w3.org/TR/annotation-vocab/}},
  note = {Accessed: 2020-02-19},
  year = {2017}
}
